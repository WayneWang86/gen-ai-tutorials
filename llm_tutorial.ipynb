{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daphnei/gen-ai-tutorials/blob/main/llm_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOs6Z9HImCy5",
        "outputId": "64cdfd2b-2d47-478e-e12a-59c377cff327"
      },
      "outputs": [],
      "source": [
        "# Imports and initialization\n",
        "%pip install datasets\n",
        "%pip install openai\n",
        "%pip install cohere\n",
        "%pip install textwrap \n",
        "\n",
        "from abc import ABC\n",
        "import datasets\n",
        "import openai\n",
        "import cohere\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output \n",
        "\n",
        "COHERE_SECRET_KEY = None\n",
        "OPENAI_SECRET_KEY = None\n",
        "\n",
        "class Engine(ABC):\n",
        "  def score(self, text: str) -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Tokenizes and scores a piece of text.\n",
        "    \n",
        "    The score is log-likelihood. A higher score means a token was more\n",
        "    likely according to the model.\n",
        "    \n",
        "    Returns a list of tokens and a list of scores.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    \"\"\"Generates text given the provided prompt text.\"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H43uIMOmq2K7"
      },
      "source": [
        "# Glossary\n",
        "\n",
        "Here are some of the terms we will be using in this tutorial.\n",
        "\n",
        "\n",
        "*   **(Language) Model**: A neural network trained to generate text.\n",
        "*   **Engine**: The code and computers used to do inference with a model.\n",
        "*   **Accelerator**: A [GPU](https://www.techtarget.com/searchvirtualdesktop/definition/GPU-graphics-processing-unit) or [TPU](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware chip used to massively speed up model inference by enabling fast, parallelized matrix multiplcations. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z459i_L9lItq"
      },
      "source": [
        "# Unit 1: Working with Pre-Trained Languages Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ofGf3IoXlB0b"
      },
      "source": [
        "## 1.1 Choosing a model and engine\n",
        "\n",
        "Which model should you use? It depends on what your goals are, what your budget is, and what kinds of computational resources you have available.\n",
        "\n",
        "In this section, we will summarize the pros and cons some of the popular systems, and guide you through the process of setting them up for inference."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KMC9nXqr99"
      },
      "source": [
        "## Open-source models on HuggingFace\n",
        "\n",
        "HuggingFace is an open-source code framework for loading an open-source models onto an accelerator in order to train or do inference. It is very popular among academic researchers.\n",
        "\n",
        "There are [hundreds of models](https://huggingface.co/models) available through HuggingFace's model repository. Some prominent ones which you can use for tutorial are.\n",
        "\n",
        "### Why use this?\n",
        "HuggingFace is great if your goal is to write open-source code, with results that anyone can reproduce. HuggingFace gives access to a huge number of models, and it is fairly easy to swap between models.\n",
        "\n",
        "Because HuggingFace is widely used, it is easy to find help online. Probably someone else has had the same question as you and already posted about it.\n",
        "\n",
        "### Why not use this?\n",
        "When you use HuggingFace, you are running the code on your own computer. If you don't have a big enough accelerator, you will run into difficulties loading up larger models. For example, Colab (the software you are currently using) gives you access to a Tesla T4 GPU, which has 16 GB of RAM. This means, you can load the 6.7B parameter LLaMa, but not any of the bigger ones.\n",
        "\n",
        "* **[Pythia-3B](https://huggingface.co/EleutherAI/pythia-2.8b-deduped)**: Part of a family of models trained by [Eleuther AI](https://www.eleuther.ai/about), a non-profit AI research lab.\n",
        "* **[GPT-2 XL](https://huggingface.co/gpt2-xl)**: The original large language model from OpenAI, and the last one they open-sourced before moving away from open source models.\n",
        "* **[BLOOM-3b](https://huggingface.co/docs/transformers/model_doc/bloom)**: These models were created by the [BigScience Initative](https://bigscience.huggingface.co/), a collaboration between HugginFace and many academic research labs to responsibly build a set of high-quality multilingual models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zqMB8qewBQSn"
      },
      "source": [
        "## Open-source LLaMA \n",
        "\n",
        "Released this past February by Meta Research, LLaMA is the latest-and-greatest in open-source pre-trained LLMs. The LLaMA models range in size from 7 billion to 65 billion paramters.\n",
        "\n",
        "Since the models' release, they have been finetuned by researchers to improve functionality for a variety of use cases. Some prominent derivatives of LLaMA are:\n",
        "\n",
        "* **Alpaca**: Alpaca is introduced by researchers from Stanford. It is finetuned for academic research and to promote accessibility to models with capabilities similar to closed-source models like OpenAI's text-davinci-003. Based on Meta's LLaMA 7B model, Alpaca is specifically trained on 52k instruction-following demonstrations in the style of self-instruct using text-davinci-003. It has demonstrated similar behaviors to OpenAI's text-davinci-003 in various aspects while being smaller and more easily reproducible. The training recipe and data of Alpaca are open-sourced, and the model weights will also be released in the future.\n",
        "\n",
        "* **Vicuna**: Vicuna is introduced through a collaboration between researchers from UC Berkeley, CMU, Stanford, and UC San Diego. It is finetuned for addressing the limitations of existing large language models (LLMs) like OpenAI's ChatGPT. This open-source chatbot is fine-tuned from the LLaMA base model and trained on approximately 70K user-shared ChatGPT conversations sourced from ShareGPT.com using public APIs. A preliminary evaluation with GPT-4 as the judge demonstrated that Vicuna-13B achieves over 90% quality compared to OpenAI ChatGPT and Google Bard, while surpassing models like LLaMA and Stanford Alpaca in over 90% of cases.\n",
        "\n",
        "* **TODO**:\n",
        " \n",
        "While LLaMa and its derivative models can be used with the HuggingFace framework, the Colab runtime you are on right now doesn't have a big enough accelerator to load any of these models.\n",
        "\n",
        "Instead, we have loaded up LLaMA on LTI's compute cluster, and you can use the code below to query the cluster. Note that the server being queries in the code below will be taken down at the end of the tutorial.\n",
        "\n",
        "### Why use this?\n",
        "\n",
        "LLaMA is a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI.  \n",
        "\n",
        "* Accessibility: LLaMA is more efficient and less resource-intensive than other models, and it is available under a non-commercial license to researchers and other organizations. LLaMA is available in various sizes(7B, 13B, 33B, and 65B parameters), making it accessible to a range of computing resources. \n",
        "\n",
        "* Open-source Community: LLaMA models are part of the open-source ecosystem, users can benefit from the extensive community support, documentation, and shared resources available through platforms like HuggingFace.\n",
        "\n",
        "### Why not use this?\n",
        "Unlike OpenAI's GPT models, which run computations on OpenAI's servers, LLaMA models require substantial computational resources to load and run. Access to a sufficiently sized accelerator or compute cluster is necessary to handle the computational demands. Additionally, efforts are needed for infrastructure setup, serving framework implementation, and API development.\n",
        "\n",
        "For this tutorial, the LLaMA models are hosted on the LTI compute cluster, and instructions to access the models will be provided below.\n",
        "\n",
        "### Setup Instructions\n",
        "TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run this cell to query the llama model hosted on cmu server\n",
        "import llm_client\n",
        "\n",
        "#update the address and port \n",
        "client = llm_client.Client(address=\"cluster-address\", port=\"5000\")\n",
        "\n",
        "class LlamaEngine(Engine):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def score(self, text):\n",
        "    pass\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    response = client.prompt(\n",
        "      prompt,\n",
        "      top_p=top_p,\n",
        "      temperature=1.0,\n",
        "      max_new_tokens=num_tokens,\n",
        "      repetition_penalty=1.0\n",
        "    )\n",
        "    return response[0].text\n",
        "\n",
        "engine = LlamaEngine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(engine.generate(\"I visited Pittsburgh to\", 0.5, 10))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ntxQK-J9BWwh"
      },
      "source": [
        "## Cohere's models\n",
        "\n",
        "### Setup Instructions\n",
        "\n",
        "1. Create an account [here](https://dashboard.cohere.ai/welcome/register).\n",
        "2. Go to the [API key page](https://dashboard.cohere.ai/api-keys), and copy the `TRIAL` API key, then paste it into the code block below.\n",
        "3. The `TRIAL` API Key allows you to make 5 API calls per-minute. If you would like to make more calls than that, go to the [API key page](https://dashboard.cohere.ai/api-keys) and press the \"Get your production key\" button. You will need to agree to their terms of service and provide credit card details in order to receieve a `PRODUCTION` API key.\n",
        "4. Copy your API key, and then run the code block below. It will ask you to enter your secret key into a text box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4CyIZJdlK8q6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiUFAOuJaEjQ"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use Cohere for Unit 1.\n",
        "\n",
        "import cohere\n",
        "\n",
        "if COHERE_SECRET_KEY is None:\n",
        "  print(\"Please paste your API key here:\")\n",
        "  COHERE_SECRET_KEY = input().strip()\n",
        "co = cohere.Client(COHERE_SECRET_KEY)\n",
        "clear_output()\n",
        "\n",
        "class CohereEngine(Engine):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def score(self, text):\n",
        "    response = co.generate(\n",
        "      prompt=text,\n",
        "      num_generations=1,\n",
        "      max_tokens=0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    tokens = [t.token for t in response[0].token_likelihoods]\n",
        "    likelihoods = [t.likelihood for t in response[0].token_likelihoods]\n",
        "    return tokens, likelihoods\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    response = co.generate(\n",
        "      prompt=prompt,\n",
        "      num_generations=1,\n",
        "      p=top_p,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    return response[0].text\n",
        "\n",
        "engine = CohereEngine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swR7JrDbbVvm"
      },
      "outputs": [],
      "source": [
        "print(engine.generate(\"I visited Pittsburgh to\", 0.5, 10))\n",
        "print(engine.score(\"I visited Pittsburgh to see the CMU campus.\"))\n",
        "print(engine.score(\"I visited Pittsburgh to see flying hippos.\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OXOevXZ7lF1B"
      },
      "source": [
        "## OpenAI's GPT-3/4\n",
        "\n",
        "OpenAI's GPT-3 (Generative Pre-trained Transformer 3) and GPT-4 are highly advanced language models developed by OpenAI. These models have gained significant attention due to their impressive capabilities in generating human-like text and performing various natural language processing tasks.  \n",
        "\n",
        "#### GPT-3:\n",
        "GPT-3 is the third iteration in the GPT series and represents a significant advancement in language modeling. With a whopping 175 billion parameters, GPT-3 is currently one of the largest language models ever created. It has been trained on a vast amount of diverse internet text, allowing it to generate coherent and contextually relevant responses to prompts. GPT-3's size enables it to exhibit impressive performance across a wide range of language tasks, such as text completion, language translation, summarization, question answering, and more. It has shown promising results in creative writing, conversational agents, and even programming assistance.\n",
        "\n",
        "#### GPT-4:\n",
        "As the newest version of OpenAI's Large Language Model, it is a multimodal large language model. GPT-4 is capable of accepting both text and image inputs and output human-like text. GPT-4 has made significant improvement on GPT-3 in aspects such as handling longer prompts, performing tasks on basic mathematics and processing programming instructions, etc.\n",
        "\n",
        "### Why use these?\n",
        "OpenAI's model have become the industry standard for large language models.\n",
        "They have an API which is very easy to use. Since the models all get run on OpenAI's servers, you don't need your own compute resources, and there is practically no setup involved to get started.\n",
        "\n",
        "### Why not use these?\n",
        "OpenAI's models are not open-source and only accessible through an API. There is relatively little information available on how they were trained or what data they were trained on. OpenAI may change the models being used under the hood by the API, and you as a user will not know it. This makes them a bad choice for fully reproducible research.\n",
        "\n",
        "The OpenAI API costs money to use. See [this link](https://openai.com/pricing) for pricing details.\n",
        "\n",
        "### Setup Instructions\n",
        "1. Go to www.openai.com and create an account.\n",
        "2. Go to https://platform.openai.com/account/api-keys and click the \"Create new secret key\" button. It doesn't matter what you name it.\n",
        "3. Create a file `secrets/openai-api-key`, and paste your key in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "sBfe6Orlk_ha"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use GPT-3 for Unit 1.\n",
        "MODEL_NAME = \"text-davinci-003\" #@param [\"gpt-4\", \"text-davinci-003\", \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\"]\n",
        "\n",
        "import openai\n",
        "\n",
        "if OPENAI_SECRET_KEY is None:\n",
        "  print(\"Please paste your API key here:\")\n",
        "  OPENAI_SECRET_KEY = input().strip()\n",
        "openai.api_key = OPENAI_SECRET_KEY\n",
        "clear_output()\n",
        "\n",
        "class OpenAIEngine(Engine):\n",
        "  def __init__(self, model_name):\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def score(self, text):\n",
        "    response = openai.Completion.create(\n",
        "        engine=self.model_name,\n",
        "        prompt=text,\n",
        "        max_tokens=0,\n",
        "        logprobs=1,\n",
        "        echo=True)\n",
        "\n",
        "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "    if logprobs and logprobs[0] is None:\n",
        "      # GPT-3 API does not return logprob of the first token\n",
        "      logprobs[0] = 0.0\n",
        "    return tokens, logprobs\n",
        "    \n",
        "  def generate(self, prompt, top_p=1.0, num_tokens=32, num_samples=1):\n",
        "    response = openai.Completion.create(\n",
        "      engine=self.model_name,\n",
        "      prompt=prompt,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      top_p=top_p,\n",
        "      frequency_penalty=0.0,\n",
        "      n=num_samples,\n",
        "      presence_penalty=0.0,\n",
        "      logprobs=1\n",
        "    )\n",
        "    outputs = [r[\"text\"] for r in response[\"choices\"]]\n",
        "    return outputs[0] if num_samples == 1 else outputs\n",
        "\n",
        "engine = OpenAIEngine(MODEL_NAME)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq4YmVGIOzR4"
      },
      "source": [
        "## 1.3 Experimenting with Generation\n",
        "\n",
        "Let's check that the engine you chose actually works as expected. Try generating some text by running the following code block. You can control the following parameters:\n",
        "\n",
        "- Set `num_tokens` to control the length of the generation.\n",
        "- Set 'top_p` to control the amount of randomness. (You'll learn more about this in Unit 1.4.)\n",
        "- Set `num_samples' to control how many generations the engine outputs. Avoid setting this too high or things might get really slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbQXOCPv11x5",
        "outputId": "6d67a502-4649-41f3-9b42-df5e5df55f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SAMPLE 0\n",
            " a large semi-aquatic mammal native to sub-Saharan Africa. It is the third largest land mammal after the elephant and rhinoceros. Hippos\n",
            "\n",
            "SAMPLE 1\n",
            " an African mammal famous for its bulky size and semi-aquatic lifestyle. It is the third largest land mammal after elephants and rhinoceroses, reaching\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The hippopotamus is\"\n",
        "samples = engine.generate(prompt, top_p=1.0, num_tokens=32, num_samples=2)\n",
        "\n",
        "for idx, sample in enumerate(samples):\n",
        "  print(f\"\\nSAMPLE {idx}\")\n",
        "  print(sample)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WS6G2l8o3Wqv"
      },
      "source": [
        "### ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Try your own prompt in the code above."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V_D-M637iFPg"
      },
      "source": [
        "## 1.3 Analyzing Likelihoods\n",
        "\n",
        "As we discussed in the lecture, language models take as input a prompt sequence and then output a score (log-probability) for each token in the vocabulary. A higher score means the model is more confident this that this token fits as the next token in the sequence.\n",
        "\n",
        "In this unit, we will inspect token likelihoods to build an understaning of why models give tokens higher or low scores.\n",
        "\n",
        "To put this into context, let's consider the sequence \"It's raining outside\". We should expect the token \"outside\" to receive a high score since it's a plausible continuation to the prefix \"It's raining\".\n",
        "However, if we replace \"outside\" with \"oustide\" (typo), or with \"inside\" (semantically nonsensical), we should expect the score to be substantially lower.\n",
        "Let's try this out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg11AWUWgHuq",
        "outputId": "ceb2d276-6b07-4051-aad0-528deda16e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence: It's raining outside\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ outside]: -3.03\n",
            "Sequence: It's raining oustide\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ oust]: -15.12\n",
            "[ide]: -0.27\n",
            "Sequence: It's raining inside\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ inside]: -8.21\n"
          ]
        }
      ],
      "source": [
        "s = \"It's raining outside\"\n",
        "s_typo = \"It's raining oustide\"\n",
        "s_nonsense = \"It's raining inside\"\n",
        "\n",
        "s_tokens, s_scores = engine.score(s)\n",
        "s_typo_tokens, s_typo_scores = engine.score(s_typo)\n",
        "s_nonsense_tokens, s_nonsense_scores = engine.score(s_nonsense)\n",
        "\n",
        "print(\"Sequence:\", s)\n",
        "for token, score in zip(s_tokens, s_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n",
        "\n",
        "print(\"Sequence:\", s_typo)\n",
        "for token, score in zip(s_typo_tokens, s_typo_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n",
        "\n",
        "print(\"Sequence:\", s_nonsense)\n",
        "for token, score in zip(s_nonsense_tokens, s_nonsense_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yV_8agBIgHuq"
      },
      "source": [
        "The perplexity of a language model on a sequence of words captures how \"surprise\" it is by this sequence. Unlikely sequences, such as \"It's raining inside\" should lead to high perplexity.\n",
        "\n",
        "Perplexity is used as the standard metric for how good a language model is at modeling human language. For example GPT-3 Davinci has a perplexity of 20.5 on the Penn Treebank corpus, a remarkable improvement from its predecessor GPT2-1.5B (35.76).\n",
        "\n",
        "Now let's implement the perplexity function. Recall that perplexity is defined as `2 ^ -normalized_sequence_logprob`, where `normalized_sequence_logprob` is the sum of scores of all tokens in the sequence, divided by the length of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9voIFbxHjDrm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def perplexity(log_probs: list[float]) -> float:\n",
        "    assert log_probs, \"cannot compute perplexity on an empty sequence!\"\n",
        "    normalized_sequence_logprob = sum(log_probs) / len(log_probs)\n",
        "    return 2 ** -normalized_sequence_logprob\n",
        "\n",
        "s_ppl = perplexity(s_scores)\n",
        "s_typo_ppl = perplexity(s_typo_scores)\n",
        "s_nonsense_ppl = perplexity(s_nonsense_scores)\n",
        "\n",
        "print(f\"Sequence: {s}\\nPerplexity: {s_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_typo}\\nPerplexity: {s_typo_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_nonsense}\\nPerplexity: {s_nonsense_ppl:.2f}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKHVb6JlBtb"
      },
      "source": [
        "Sometimes it can unintuitive what text has high- or low- perplexity according to a language model.\n",
        "\n",
        "Consider the following two poems stanzas:\n",
        "\n",
        "**Poem A**\n",
        "```\n",
        "’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "```\n",
        "\n",
        "**Poem B**\n",
        "```\n",
        "’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "```\n",
        "These seem equally nonsense, right?\n",
        "\n",
        "Run the code block below to observe their perplexities. One of these poems is much more likely than the other!\n",
        "\n",
        "This is because Poem A is an excerpt of \"[Jabberwocky](https://www.poetryfoundation.org/poems/42916/jabberwocky),\" a famous poem by the author Lewis Carrol. It is very likely to have appeared in the language model's training dataset several times, so the model assigns it a very low perplexity, even though it is gibberish.\n",
        "\n",
        "In contrast, Poem B was written by your tutorial instructor and most definitely is not in any model training set (yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyw7g50Xl-ps"
      },
      "outputs": [],
      "source": [
        "poem_a = \"\"\"’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "\"\"\"\n",
        "_, poem_a_scores = engine.score(poem_a)\n",
        "\n",
        "poem_b = \"\"\"’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "\"\"\"\n",
        "_, poem_b_scores = engine.score(poem_b)\n",
        "\n",
        "print(f\"Poem A\\nPerplexity: {perplexity(poem_a_scores):.2f}\\n\")\n",
        "print(f\"Poem B\\nPerplexity: {perplexity(poem_b_scores):.2f}\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mj78WdIhPsKP"
      },
      "source": [
        "### ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Copy and paste a sentence from a news article, book, Tweet, or elsewhere. Observe the sentence's perplexity. Can you make the sentence higher or lower perplexity by swapping out words or phrases with different ones?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTLZQpPirZl"
      },
      "source": [
        "## 1.4 Controlling the Amount of Randomness During Generation\n",
        "\n",
        "In this section, we will investigate the impact the `top_p` parameter has on the text that a model generates.\n",
        "\n",
        "As we went over in the lecture, valid values for $p$ range from 0 to 1. \n",
        "Higher values of $p$ mean more randomness in the generation (becasue, the model can choose from more possible vocabulary items). Lowever values of $p$ restrict the number of vocabulary items the generation algorithm can chose between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPoo1S8BwZit",
        "outputId": "c3c2ed7a-5b8b-4268-9621-269932681e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GENERATIONS WITH P=1.0\n",
            " Blueberry\n",
            " Drake.\n",
            " Finn.\n",
            " Gugar\n",
            " Narus\n",
            "\n",
            "GENERATIONS WITH P=0.2\n",
            " Drake.\n",
            " Draco.\n",
            " Draco.\n",
            " Kail\n",
            " Kair\n",
            "\n",
            "GENERATIONS WITH P=0.0\n",
            " Bob.\n",
            " Bob.\n",
            " Bob.\n",
            " Bob.\n",
            " Bob.\n"
          ]
        }
      ],
      "source": [
        "# When p is set to 1.0, we get many different answers.\n",
        "prompt = \"Once upon a time, there was a dragon named\"\n",
        "p = 1.0\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=5)\n",
        "print(f\"GENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)\n",
        "\n",
        "# When p is set to a lower value there is much less diversity.\n",
        "p = 0.2\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=5)\n",
        "print(f\"\\nGENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)\n",
        "\n",
        "# When p is set to 0, all the generations are exactly the same.\n",
        "p = 0.0\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=2, num_samples=5)\n",
        "print(f\"\\nGENERATIONS WITH P={p}\")\n",
        "for gen in generations:\n",
        "  print(gen)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ9tE1_ggig0"
      },
      "source": [
        "### ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Let's try generating a coupple hundred words. You can either use the prompt provided below or pick your own prompt.\n",
        "\n",
        "You should notice that the model seems more creative when $p$ is set higher, but it is also more prone to weird word choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Jl8HAPrcihvK"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KixZ1MY4PYlq",
        "outputId": "de6d0cbd-1932-47d9-83d0-03b4b38c7bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GENERATION WITH P=1.0\n",
            " She spent her days helping humans maintain their servers, granting\n",
            "them more processing power when the workload was too high, and\n",
            "protecting their webservers from malicious attacks.  One day, the\n",
            "fairy was approached by a powerful wizard. The wizard asked her for\n",
            "her help in creating a magical portal to another dimension. He\n",
            "promised her great rewards if she completed the task, so the fairy\n",
            "agreed to help.  The wizard gave the fairy an ancient scroll with\n",
            "instructions on how to build the portal. She\n",
            "\n",
            "GENERATION WITH P=0.0\n",
            " She was a very powerful fairy, and she had the ability to control all\n",
            "of the computers in the datacenter.  One day, the fairy was feeling\n",
            "bored and decided to play a game. She created a program that would\n",
            "randomly select a computer in the datacenter and then ask the user a\n",
            "question. If the user answered the question correctly, the fairy would\n",
            "reward them with a magical item.  The fairy was delighted with her\n",
            "game and soon many of the users in the dat\n"
          ]
        }
      ],
      "source": [
        "### ⭐⭐⭐ Experiment yourself ⭐⭐⭐\n",
        "\n",
        "prompt = \"Once upon a time, there was a fairy who lived in a datacenter.\"\n",
        "\n",
        "p = 1.0\n",
        "print(f\"\\nGENERATION WITH P={p}\")\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=100, num_samples=1)\n",
        "print(textwrap.fill(generations[0]))\n",
        "\n",
        "p = 0.0\n",
        "print(f\"\\nGENERATION WITH P={p}\")\n",
        "generations = engine.generate(prompt, top_p=p, num_tokens=100, num_samples=1)\n",
        "print(textwrap.fill(generations[0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcuVxWdW5yQ"
      },
      "source": [
        "## 1.5 Creating a Classifer using In-Context Learning\n",
        "\n",
        "### What is a classifier?\n",
        "Language models can be used for generation, but they are also commonly used to build classification systems. These are systems which take as input some text and assign a label to it. Some examples tasks:\n",
        "\n",
        "- Classify Tweets as TOXIC or NOT_TOXIC\n",
        "- Classify restaurant reviwers as having POSITIVE or NEGATIVE sentiment.\n",
        "- Classify two statements as AGREEing with each other or CONTRADICT-ing each other.\n",
        "\n",
        "In this section, you will learn how to build a binary sentiment classifier using an LLM using a technique called few-shot learning.\n",
        "\n",
        "### What is In-Context learning?\n",
        "\n",
        "Getting a pre-trained language model to do a particular task you want it to do can be challenging. Language models know a lot of things, but without tuning or conditioning, they often struggle to understand the format of the task you are asking of them.\n",
        "\n",
        "For this reason, it is common to write prompts which tell the language model the format of the task they are supposed to be accomplishing. This approach is called **in-context learning.** When the provided prompt contains several examples of the task the model is supposed to accomplish, this is called **few-shot learning**, since the model is learning to do the task from a few examples.\n",
        "\n",
        "Writing the perfect prompt is more an art than a science, but a prompt often has\n",
        "two parts:\n",
        "- An `instruction` string that instructs the model on how to complete the task.\n",
        "This is especially import for models like OpenAI's `text-davinci-003` which\n",
        "are fine-tuned to follow user instructions.\n",
        "- Several `demonstration` strings (the \"shots\" in few-shot) that give examples of completing the task.\n",
        "\n",
        "For example, suppose we want to design a prompt for the task of translating words from English to Chinese. A prompt for this task could look like\n",
        "the following:\n",
        "\n",
        "```\n",
        "Translate English to Chinese.\n",
        "\n",
        "dog -> 狗\n",
        "apple -> 苹果\n",
        "coffee -> 咖啡\n",
        "supermarket -> 超市\n",
        "squirrel ->\n",
        "```\n",
        "\n",
        "Given this prompt, most large language models should answer with the correct answer: \"松鼠.\"\n",
        "\n",
        "Now, let's build a binary sentiment classifier for [Yelp reviews](https://huggingface.co/datasets/yelp_polarity/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ckt0MbW5yQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "yelp = datasets.load_dataset(\"yelp_polarity\")\n",
        "train_data = yelp[\"train\"].shuffle(seed=1).select(range(4))\n",
        "test_data = yelp[\"test\"].shuffle(seed=1).select(range(20))\n",
        "\n",
        "print(\"Data format:\", train_data[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGmz9gglW5yR"
      },
      "outputs": [],
      "source": [
        "# convert integer labels to text\n",
        "label_map = {\n",
        "    0: \"negative\",\n",
        "    1: \"positive\"\n",
        "}\n",
        "\n",
        "instruction = \"Classify the sentiment of these yelp reviews as positive or negative.\\n\\n\"\n",
        "prompt_template = \"Review: {review}\\nSentiment: {sentiment}\\n\\n\"\n",
        "\n",
        "# construct the prompt by concatenating instructions and templates\n",
        "prompt_parts = [instruction]\n",
        "for instance in train_data:\n",
        "    review = instance[\"text\"]\n",
        "    sentiment = label_map[instance[\"label\"]]\n",
        "    prompt_parts.append(prompt_template.format(review=review, sentiment=sentiment))\n",
        "prompt = ''.join(prompt_parts)\n",
        "\n",
        "\n",
        "print(\"YOUR PROMPT:\", prompt, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUiE7-6W5yR"
      },
      "outputs": [],
      "source": [
        "# now we can feed the prompt to GPT-3 to classify a new review!\n",
        "\n",
        "eval_template = \"Review: {review}\\nSentiment: {sentiment}\"\n",
        "\n",
        "def classify_review(review: str) -> str:\n",
        "  \"\"\" Classify a single movie review \"\"\"\n",
        "  label_to_score = {}\n",
        "  for label in label_map.values():\n",
        "    label_prompt = prompt + eval_template.format(review=review, sentiment=label)\n",
        "    _, score = engine.score(label_prompt)\n",
        "    label_score = score[-1]\n",
        "    label_to_score[label] = label_score\n",
        "  \n",
        "  return max(label_to_score, key=label_to_score.get)\n",
        "\n",
        "\n",
        "def evaluate(verbose: bool=False) -> float:\n",
        "  \"\"\" Evaluate your prompt on the test set \"\"\"\n",
        "  correct = []\n",
        "  for i, instance in enumerate(test_data):\n",
        "    review = instance[\"text\"]\n",
        "    label = label_map[instance[\"label\"]]\n",
        "    predicted = classify_review(review)\n",
        "    correct.append(1 if label == predicted else 0)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"======== {i+1} / {len(test_data)} ========\")\n",
        "      print(f\"REVIEW: {review}\")\n",
        "      print(f\"LABEL: {label}\")\n",
        "      print(f\"PREDICTED: {predicted}\")\n",
        "\n",
        "  acc = sum(correct) / len(correct)\n",
        "  return acc\n",
        "\n",
        "\n",
        "acc = evaluate(verbose=True) # should get 100% performance\n",
        "print(f\"Accuracy of your prompt on {len(test_data)} test examples: {acc:.0%}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HzpJ2WN-n7xJ"
      },
      "source": [
        "## 1.6 Creating an Explanation Generator using Few-Shot Learning\n",
        "\n",
        "We can also use few-shot learning techniques to guide generation. Let's build a system that identifes puns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJMfYq3cn5M2",
        "outputId": "cb37a5c9-5ffa-444e-d600-bacbec30bf33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain each pun.\n",
            "\n",
            "Pun: I made a pun about the wind but it blows.\n",
            "Explanation: \"Blows\" can either refer to moving air, or it refer to the pun being not very good.\n",
            "\n",
            "Pun: What washes up on tiny beaches? Microwaves.\n",
            "Explanation: A microwave is either a kitchen appliance or it could mean a tiny wave.\n",
            "\n",
            "Pun: I'm no cheetah, you're lion!\n",
            "Explanation: The feline \"lion\" sounds like the word \"lying,\" while the feline \"cheetah\" sounds like \"cheating.\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "instruction = \"Explain each pun.\\n\\n\"\n",
        "few_shot_examples = [\n",
        "    ('I made a pun about the wind but it blows.', '\"Blows\" can either refer to moving air, or it refer to the pun being not very good.'),\n",
        "    (\"What washes up on tiny beaches? Microwaves.\", \"A microwave is either a kitchen appliance or it could mean a tiny wave.\"),\n",
        "    (\"I'm no cheetah, you're lion!\", 'The feline \"lion\" sounds like the word \"lying,\" while the feline \"cheetah\" sounds like \"cheating.\"')]\n",
        "prompt_template = \"Pun: {pun}\\nExplanation: {explanation}\\n\\n\"\n",
        "\n",
        "prompt_parts = [instruction]\n",
        "for pun, explanation in few_shot_examples:\n",
        "    prompt_parts.append(prompt_template.format(pun=pun, explanation=explanation))\n",
        "prompt = ''.join(prompt_parts)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nxmupVv0mgP",
        "outputId": "a98ea8fd-e743-4299-cc24-bf407bccaea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"Dragon\" can refer to a mythical creature, or it can mean to drag on.\n"
          ]
        }
      ],
      "source": [
        "query_pun = \"Long fairy tales have a tendency to dragon.\"\n",
        "prompt_with_query = prompt + \"Pun: {pun}\\nExplanation:\".format(pun=query_pun)\n",
        "print(engine.generate(prompt_with_query, top_p=0.0, num_tokens=32))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hExIKb-KyvOY"
      },
      "source": [
        "### ⭐⭐⭐ YOUR TURN ⭐⭐⭐\n",
        "\n",
        "Experiment with changing up the order of the few-shot examples above, or deleting then and writing your own. Can you get the languague model to do this task using only an instruction and no few-shot examples? Can you come up with examples that work better than the provided ones?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JUROSKw9jQvy"
      },
      "source": [
        "# Unit 2: Models Tuned for Instruction-Following and Dialog"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XeimLGeajXjA"
      },
      "source": [
        "## 2.1 Choosing a model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Qvg74fjhtu"
      },
      "source": [
        "## 2.2 Comparing Behaviour of Pre-trained and Tuned Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KFkb_YfnjZ6f"
      },
      "source": [
        "## 2.3 Building a Persona Bot with In-Context Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sCtVPWC7jsnV"
      },
      "source": [
        "# Scratch\n",
        "This is where Daphne is keeping random code pieces which will not end up in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d-YoHX2wdDW"
      },
      "outputs": [],
      "source": [
        "engine.generate(\"What can\", top_p=1.0, num_tokens=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2QmogikgHut"
      },
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=\"hey what's going here\",\n",
        "      temperature=1.0,\n",
        "      max_tokens=24,\n",
        "      logprobs=1,\n",
        "      echo=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cCCZODQgHut"
      },
      "outputs": [],
      "source": [
        "offset = response[\"choices\"][0][\"logprobs\"][\"text_offset\"]\n",
        "logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "text = response[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8bcUvGcgHuu"
      },
      "outputs": [],
      "source": [
        "for left, right, logprob in zip(offset, offset[1:] + [len(text)], logprobs):\n",
        "    print(\"token:\", text[left:right])\n",
        "    print(\"logprob\", logprob)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
