{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daphnei/gen-ai-tutorials/blob/main/llm_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bOs6Z9HImCy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d25bc8-c378-4499-e7db-164dffc8ae54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.12.0 dill-0.3.6 huggingface-hub-0.15.1 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.7.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.4)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.27.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cohere) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cohere) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cohere) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# Imports and initialization\n",
        "%pip install datasets\n",
        "%pip install openai\n",
        "%pip install cohere\n",
        "\n",
        "from abc import ABC\n",
        "import datasets\n",
        "import openai\n",
        "import cohere\n",
        "\n",
        "\n",
        "class Engine(ABC):\n",
        "  def score(self, text: str) -> tuple[list[str], list[float]]:\n",
        "    \"\"\"Tokenizes and scores a piece of text.\n",
        "    \n",
        "    The score is log-likelihood. A higher score means a token was more\n",
        "    likely according to the model.\n",
        "    \n",
        "    Returns a list of tokens and a list of scores.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    \"\"\"Generates text given the provided prompt text.\"\"\"\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H43uIMOmq2K7"
      },
      "source": [
        "# Glossary\n",
        "\n",
        "Here are some of the terms we will be using in this tutorial.\n",
        "\n",
        "\n",
        "*   **(Language) Model**: A neural network trained to generate text.\n",
        "*   **Engine**: The code and computers used to do inference with a model.\n",
        "*   **Accelerator**: A [GPU](https://www.techtarget.com/searchvirtualdesktop/definition/GPU-graphics-processing-unit) or [TPU](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware chip used to massively speed up model inference by enabling fast, parallelized matrix multiplcations. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z459i_L9lItq"
      },
      "source": [
        "# Unit 1: Working with Pre-Trained Languages Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofGf3IoXlB0b"
      },
      "source": [
        "## 1.1 Choosing a model and engine\n",
        "\n",
        "Which model should you use? It depends on what your goals are, what your budget is, and what kinds of computational resources you have available.\n",
        "\n",
        "In this section, we will summarize the pros and cons some of the popular systems, and guide you through the process of setting them up for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KMC9nXqr99"
      },
      "source": [
        "## Open-source models on HuggingFace\n",
        "\n",
        "HuggingFace is an open-source code framework for loading an open-source models onto an accelerator in order to train or do inference. It is very popular among academic researchers.\n",
        "\n",
        "There are [hundreds of models](https://huggingface.co/models) available through HuggingFace's model repository. Some prominent ones which you can use for tutorial are.\n",
        "\n",
        "### Why use this?\n",
        "HuggingFace is great if your goal is to write open-source code, with results that anyone can reproduce. HuggingFace gives access to a huge number of models, and it is fairly easy to swap between models.\n",
        "\n",
        "Because HuggingFace is widely used, it is easy to find help online. Probably someone else has had the same question as you and already posted about it.\n",
        "\n",
        "### Why not use this?\n",
        "When you use HuggingFace, you are running the code on your own computer. If you don't have a big enough accelerator, you will run into difficulties loading up larger models. For example, Colab (the software you are currently using) gives you access to a Tesla T4 GPU, which has 16 GB of RAM. This means, you can load the 6.7B parameter LLaMa, but not any of the bigger ones.\n",
        "\n",
        "* **[Pythia-3B](https://huggingface.co/EleutherAI/pythia-2.8b-deduped)**: Part of a family of models trained by [Eleuther AI](https://www.eleuther.ai/about), a non-profit AI research lab.\n",
        "* **[GPT-2 XL](https://huggingface.co/gpt2-xl)**: The original large language model from OpenAI, and the last one they open-sourced before moving away from open source models.\n",
        "* **[BLOOM-3b](https://huggingface.co/docs/transformers/model_doc/bloom)**: These models were created by the [BigScience Initative](https://bigscience.huggingface.co/), a collaboration between HugginFace and many academic research labs to responsibly build a set of high-quality multilingual models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqMB8qewBQSn"
      },
      "source": [
        "## Open-source LLaMA \n",
        "\n",
        "Released this past February by Meta Research, LLaMA is the latest-and-greatest in open-source pre-trained LLMs. The LLaMA models range in size from 7 billion to 65 billion paramters.\n",
        "\n",
        "Since the models' release, they have been finetuned by researchers to improve  functionality for a variety of use cases. Some prominent derivatives of LLaMA are:\n",
        "\n",
        "* **Vicuna**: Finetuned for TODO\n",
        "* **Alpaca**: Finetuned for TODO\n",
        "* **TODO**:\n",
        " \n",
        "While LLaMa and its derivative models can be used with the HuggingFace framework, the Colab runtime you are on right now doesn't have a big enough accelerator to load any of these models.\n",
        "\n",
        "Instead, we have loaded up LLaMA on LTI's compute cluster, and you can use the code below to query the cluster. Note that the server being queries in the code below will be taken down at the end of the tutorial.\n",
        "\n",
        "### Why use this?\n",
        "TODO\n",
        "\n",
        "### Why not use this?\n",
        "TODO\n",
        "\n",
        "### Setup Instructions\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntxQK-J9BWwh"
      },
      "source": [
        "## Cohere's models\n",
        "\n",
        "### Setup Instructions\n",
        "\n",
        "1. Create an account [here](https://dashboard.cohere.ai/welcome/register).\n",
        "2. Go to the [API key page](https://dashboard.cohere.ai/api-keys), and copy the `TRIAL` API key, then paste it into the code block below.\n",
        "3. The `TRIAL` API Key allows you to make 5 API calls per-minute. If you would like to make more calls than that, go to the [API key page](https://dashboard.cohere.ai/api-keys) and press the \"Get your production key\" button. You will need to agree to their terms of service and provide credit card details in order to receieve a `PRODUCTION` API key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this code block to use Cohere for Unit 1.\n",
        "SECRET_KEY = \"Paste your secret key here.\" #@param {type:\"string\"}\n",
        "\n",
        "!pip install cohere\n",
        "import cohere\n",
        "co = cohere.Client(SECRET_KEY)\n",
        "\n",
        "class CohereEngine(Engine):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def score(self, text):\n",
        "    response = co.generate(\n",
        "      prompt=text,\n",
        "      num_generations=1,\n",
        "      max_tokens=0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    tokens = [t.token for t in response[0].token_likelihoods]\n",
        "    likelihoods = [t.likelihood for t in response[0].token_likelihoods]\n",
        "    return tokens, likelihoods\n",
        "\n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    response = co.generate(\n",
        "      prompt=prompt,\n",
        "      num_generations=1,\n",
        "      p=top_p,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0,\n",
        "      return_likelihoods=\"ALL\"\n",
        "    )\n",
        "    return response[0].text\n",
        "\n",
        "engine = CohereEngine()"
      ],
      "metadata": {
        "id": "JiUFAOuJaEjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(engine.generate(\"I visited Pittsburgh to\", 0.5, 10))\n",
        "print(engine.score(\"I visited Pittsburgh to see the CMU campus.\"))\n",
        "print(engine.score(\"I visited Pittsburgh to see flying hippos.\"))"
      ],
      "metadata": {
        "id": "swR7JrDbbVvm",
        "outputId": "5d1ba090-8b3a-4547-d183-318a62e14bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "see the Cathedral of Learning at the University of\n",
            "[('<BOS_TOKEN>', 0), ('I', -3.5298903), (' visited', -6.9353056), (' Pittsburgh', -8.640782), (' to', -3.871383), (' see', -1.9880424), (' the', -1.5376686), (' CMU', -9.146852), (' campus', -3.5548153), ('.', -2.5740197), ('<EOP_TOKEN>', -19.812128)]\n",
            "[('<BOS_TOKEN>', 0), ('I', -3.5298903), (' visited', -6.9353056), (' Pittsburgh', -8.640782), (' to', -3.871383), (' see', -1.9880424), (' flying', -12.521135), (' hipp', -12.904638), ('os', -0.5945612), ('.', -1.4367169), ('<EOP_TOKEN>', -20.154535)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXOevXZ7lF1B"
      },
      "source": [
        "## OpenAI's GPT-3/4\n",
        "\n",
        "TODO: Add brief summary of these models\n",
        "\n",
        "### Why use these?\n",
        "OpenAI's model have become the industry standard for large language models.\n",
        "They have an API which is very easy to use. Since the models all get run on OpenAI's servers, you don't need your own compute resources, and there is practically no setup involved to get started.\n",
        "\n",
        "### Why not use these?\n",
        "OpenAI's models are not open-source and only accessible through an API. There is relatively little information available on how they were trained or what data they were trained on. OpenAI may change the models being used under the hood by the API, and you as a user will not know it. This makes them a bad choice for fully reproducible research.\n",
        "\n",
        "The OpenAI API costs money to use. See [this link](https://openai.com/pricing) for pricing details.\n",
        "\n",
        "### Setup Instructions\n",
        "1. Go to www.openai.com and create an account.\n",
        "2. Go to https://platform.openai.com/account/api-keys and click the \"Create new secret key\" button. It doesn't matter what you name it.\n",
        "3. Copy the secret key and paste it into the code block below. Then run the code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sBfe6Orlk_ha"
      },
      "outputs": [],
      "source": [
        "#@title Run this code block to use GPT-3 for Unit 1.\n",
        "MODEL_NAME = \"text-davinci-003\" #@param [\"gpt-4\", \"text-davinci-003\", \"text-curie-001\", \"text-babbage-001\", \"text-ada-001\"]\n",
        "SECRET_KEY = \"PASTE YOUR KEY HERE\" #@param {type:\"string\"}\n",
        "\n",
        "openai.api_key = SECRET_KEY\n",
        "\n",
        "class OpenAIEngine(Engine):\n",
        "  def __init__(self, model_name):\n",
        "    self.model_name = model_name\n",
        "\n",
        "  def score(self, text):\n",
        "    response = openai.Completion.create(\n",
        "        engine=self.model_name,\n",
        "        prompt=text,\n",
        "        max_tokens=0,\n",
        "        logprobs=1,\n",
        "        echo=True)\n",
        "\n",
        "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "    if logprobs and logprobs[0] is None:\n",
        "      # GPT-3 API does not return logprob of the first token\n",
        "      logprobs[0] = 0.0\n",
        "    return tokens, logprobs\n",
        "    \n",
        "  def generate(self, prompt, top_p, num_tokens):\n",
        "    response = openai.Completion.create(\n",
        "      engine=self.model_name,\n",
        "      prompt=prompt,\n",
        "      temperature=1.0,\n",
        "      max_tokens=num_tokens,\n",
        "      top_p=top_p,\n",
        "      frequency_penalty=0.0,\n",
        "      presence_penalty=0.0,\n",
        "      logprobs=1\n",
        "    )\n",
        "    return response[\"choices\"][0][\"text\"]\n",
        "\n",
        "engine = OpenAIEngine(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(engine.score(\"I visited Pittsburgh to see the CMU campus.\"))\n"
      ],
      "metadata": {
        "id": "no2CgnghdIeq",
        "outputId": "5139236e-3967-43c5-b1f8-7019e315118b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['I', ' visited', ' Pittsburgh', ' to', ' see', ' the', ' CM', 'U', ' campus', '.'], [0.0, -9.353717, -8.062071, -4.9968047, -1.0943034, -0.90393007, -9.247493, -0.0022635593, -0.785396, -3.3115568])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "en"
      ],
      "metadata": {
        "id": "SPjWeGxLdBf8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_D-M637iFPg"
      },
      "source": [
        "## 1.3 Analyzing Likelihoods\n",
        "\n",
        "As we discussed in the lecture, language models take as input a prompt sequence and then output a score (log-probability) for each token in the vocabulary. A higher score means the model is more confident this that this token fits as the next token in the sequence.\n",
        "\n",
        "In this unit, we will inspect token likelihoods to build an understaning of why models give tokens higher or low scores.\n",
        "\n",
        "To put this into context, let's consider the sequence \"It's raining outside\". We should expect the token \"outside\" to receive a high score since it's a plausible continuation to the prefix \"It's raining\".\n",
        "However, if we replace \"outside\" with \"oustide\" (typo), or with \"inside\" (semantically nonsensical), we should expect the score to be substantially lower.\n",
        "Let's try this out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg11AWUWgHuq",
        "outputId": "18e19977-56f4-4f92-97f0-54f68fd35ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: It's raining outside\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ outside]: -3.03\n",
            "Sequence: It's raining oustide\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ oust]: -15.12\n",
            "[ide]: -0.27\n",
            "Sequence: It's raining inside\n",
            "[It]: 0.00\n",
            "['s]: -1.78\n",
            "[ raining]: -6.21\n",
            "[ inside]: -8.22\n"
          ]
        }
      ],
      "source": [
        "s = \"It's raining outside\"\n",
        "s_typo = \"It's raining oustide\"\n",
        "s_nonsense = \"It's raining inside\"\n",
        "\n",
        "s_tokens, s_scores = engine.score(s)\n",
        "s_typo_tokens, s_typo_scores = engine.score(s_typo)\n",
        "s_nonsense_tokens, s_nonsense_scores = engine.score(s_nonsense)\n",
        "\n",
        "print(\"Sequence:\", s)\n",
        "for token, score in zip(s_tokens, s_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n",
        "\n",
        "print(\"Sequence:\", s_typo)\n",
        "for token, score in zip(s_typo_tokens, s_typo_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n",
        "\n",
        "print(\"Sequence:\", s_nonsense)\n",
        "for token, score in zip(s_nonsense_tokens, s_nonsense_scores):\n",
        "    print(f\"[{token}]: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV_8agBIgHuq"
      },
      "source": [
        "The perplexity of a language model on a sequence of words captures how \"surprise\" it is by this sequence. Unlikely sequences, such as \"It's raining inside\" should lead to high perplexity.\n",
        "\n",
        "Perplexity is used as the standard metric for how good a language model is at modeling human language. For example GPT-3 Davinci has a perplexity of 20.5 on the Penn Treebank corpus, a remarkable improvement from its predecessor GPT2-1.5B (35.76).\n",
        "\n",
        "Now let's implement the perplexity function. Recall that perplexity is defined as `2 ^ -normalized_sequence_logprob`, where `normalized_sequence_logprob` is the sum of scores of all tokens in the sequence, divided by the length of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9voIFbxHjDrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd45ef2-6bed-4e4c-b6d8-6ed1baeb65f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: It's raining outside\n",
            "Perplexity: 6.75\n",
            "\n",
            "Sequence: It's raining oustide\n",
            "Perplexity: 25.57\n",
            "\n",
            "Sequence: It's raining inside\n",
            "Perplexity: 16.58\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def perplexity(log_probs: list[float]) -> float:\n",
        "    assert log_probs, \"cannot compute perplexity on an empty sequence!\"\n",
        "    normalized_sequence_logprob = sum(log_probs) / len(log_probs)\n",
        "    return 2 ** -normalized_sequence_logprob\n",
        "\n",
        "s_ppl = perplexity(s_scores)\n",
        "s_typo_ppl = perplexity(s_typo_scores)\n",
        "s_nonsense_ppl = perplexity(s_nonsense_scores)\n",
        "\n",
        "print(f\"Sequence: {s}\\nPerplexity: {s_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_typo}\\nPerplexity: {s_typo_ppl:.2f}\\n\")\n",
        "print(f\"Sequence: {s_nonsense}\\nPerplexity: {s_nonsense_ppl:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes it can unintuitive what text has high- or low- perplexity according to a language model.\n",
        "\n",
        "Consider the following two poems stanzas:\n",
        "\n",
        "**Poem A**\n",
        "```\n",
        "’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "```\n",
        "\n",
        "**Poem B**\n",
        "```\n",
        "’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "```\n",
        "These seem equally nonsense, right?\n",
        "\n",
        "Run the code block below to observe their perplexities. One of these poems is much more likely than the other!\n",
        "\n",
        "This is because Poem A is an excerpt of \"[Jabberwocky](https://www.poetryfoundation.org/poems/42916/jabberwocky),\" a famous poem by the author Lewis Carrol. It is very likely to have appeared in the language model's training dataset several times, so the model assigns it a very low perplexity, even though it is gibberish.\n",
        "\n",
        "In contrast, Poem B was written by your tutorial instructor and most definitely is not in any model training set (yet)."
      ],
      "metadata": {
        "id": "PeKHVb6JlBtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem_a = \"\"\"’Twas brillig, and the slithy toves\n",
        "      Did gyre and gimble in the wabe:\n",
        "All mimsy were the borogoves,\n",
        "      And the mome raths outgrabe.\n",
        "\"\"\"\n",
        "_, poem_a_scores = engine.score(poem_a)\n",
        "\n",
        "poem_b = \"\"\"’Twas crillig, and the brithy lokes\n",
        "      Did ryne and jimble in the waze:\n",
        "All timsy were the habogroves,\n",
        "      And the nome paths intrabe.\n",
        "\"\"\"\n",
        "_, poem_b_scores = engine.score(poem_b)\n",
        "\n",
        "print(f\"Poem A: {s}\\nPerplexity: {perplexity(poem_a_scores):.2f}\\n\")\n",
        "print(f\"Poem B: {s}\\nPerplexity: {perplexity(poem_b_scores):.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyw7g50Xl-ps",
        "outputId": "1aa341b9-00b1-4f5a-8a39-ee13d0f7c929"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poem A: It's raining outside\n",
            "Perplexity: 1.57\n",
            "\n",
            "Poem B: It's raining outside\n",
            "Perplexity: 14.11\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTLZQpPirZl"
      },
      "source": [
        "## 1.4 Controlling the Amount of Randomness During Generation\n",
        "\n",
        "In this section, we will investigate the impact the `top_p` parameter has on the text that a model generates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPoo1S8BwZit"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfcuVxWdW5yQ"
      },
      "source": [
        "## 1.5 Creating a Classifer using Few-Shot Learning\n",
        "\n",
        "In this section, we will build a binary sentiment classifier using a LLM such\n",
        "as GPT-3.\n",
        "\n",
        "Writing the perfect prompt is more an art than a science, but a prompt often has\n",
        "two parts:\n",
        "- An `instruction` string that instructs the model on how to complete the task.\n",
        "Adding it is often effective, because models such as OpenAI's `text-davinci-003`\n",
        "are fine-tuned to follow user instructions.\n",
        "- Several `demonstration` strings that give examples of completing the task.\n",
        "\n",
        "For example, a prompt that translates English words to Chinese could look like\n",
        "the following. Running GPT-3 to complete the prompt provides the correct answer\n",
        "\"松鼠\".\n",
        "\n",
        "```\n",
        "Translate English to Chinese.\n",
        "\n",
        "dog -> 狗\n",
        "apple -> 苹果\n",
        "coffee -> 咖啡\n",
        "supermarket -> 超市\n",
        "squirrel ->\n",
        "```\n",
        "\n",
        "Now, let's build a binary sentiment classifier for [yelp reviews](https://huggingface.co/datasets/yelp_polarity/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33ckt0MbW5yQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "yelp = datasets.load_dataset(\"yelp_polarity\")\n",
        "train_data = yelp[\"train\"].shuffle(seed=1).select(range(4))\n",
        "test_data = yelp[\"test\"].shuffle(seed=1).select(range(20))\n",
        "\n",
        "print(\"Data format:\", train_data[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGmz9gglW5yR"
      },
      "outputs": [],
      "source": [
        "# convert integer labels to text\n",
        "label_map = {\n",
        "    0: \" negative\",\n",
        "    1: \" positive\"\n",
        "}\n",
        "\"\"\"\n",
        "It might seem strange to put a space before \"negative\". This is due to how\n",
        "GPT-3's tokenizer (BPE) works: \" negative\" itself is one token, and is observed\n",
        "much more frequently than \"negative\" without the whitespace during training.\n",
        "\"\"\"\n",
        "\n",
        "instruction = \"Classify the sentiment of these yelp reviews as positive or negative.\\n\\n\"\n",
        "demo_template = \"Review: {review}\\nSentiment:{sentiment}\\n\\n\"\n",
        "\n",
        "# construct the prompt by concatenating instructions and templates\n",
        "prompt_parts = [instruction]\n",
        "for instance in train_data:\n",
        "    review = instance[\"text\"]\n",
        "    sentiment = label_map[instance[\"label\"]]\n",
        "    prompt_parts.append(demo_template.format(review=review, sentiment=sentiment))\n",
        "prompt = ''.join(prompt_parts)\n",
        "\n",
        "\n",
        "print(\"YOUR PROMPT:\", prompt, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okUiE7-6W5yR"
      },
      "outputs": [],
      "source": [
        "# now we can feed the prompt to GPT-3 to classify a new review!\n",
        "\n",
        "eval_template = \"Review: {review}\\nSentiment:\"\n",
        "\n",
        "def classify_review(review: str) -> str:\n",
        "  \"\"\" Classify a single movie review \"\"\"\n",
        "  classify_prompt = prompt + eval_template.format(review=review)\n",
        "  response = openai.Completion.create(\n",
        "    engine=MODEL_NAME,\n",
        "    prompt=classify_prompt,\n",
        "    temperature=0.0, # <- only need the top-probability answer for classification\n",
        "    max_tokens=1,\n",
        "    logit_bias={\n",
        "      \"3967\": 100,\n",
        "      \"4633\": 100,\n",
        "    } # <- whitelist only tokens that correspond to a label\n",
        "  )\n",
        "  return response[\"choices\"][0][\"text\"]\n",
        "\n",
        "\n",
        "def evaluate(verbose: bool=False) -> float:\n",
        "  \"\"\" Evaluate your prompt on the test set \"\"\"\n",
        "  correct = []\n",
        "  for i, instance in enumerate(test_data):\n",
        "    review = instance[\"text\"]\n",
        "    label = label_map[instance[\"label\"]]\n",
        "    predicted = classify_review(review)\n",
        "    correct.append(1 if label == predicted else 0)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"======== {i+1} / {len(test_data)} ========\")\n",
        "      print(f\"REVIEW: {review}\")\n",
        "      print(f\"LABEL: {label}\")\n",
        "      print(f\"PREDICTED: {predicted}\")\n",
        "\n",
        "  acc = sum(correct) / len(correct)\n",
        "  return acc\n",
        "\n",
        "\n",
        "acc = evaluate(verbose=True) # should get 100% performance\n",
        "print(f\"Accuracy of your prompt on {len(test_data)} test examples: {acc:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUROSKw9jQvy"
      },
      "source": [
        "# Unit 2: Models Tuned for Instruction-Following and Dialog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeimLGeajXjA"
      },
      "source": [
        "## 2.1 Choosing a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Qvg74fjhtu"
      },
      "source": [
        "## 2.2 Comparing Behaviour of Pre-trained and Tuned Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFkb_YfnjZ6f"
      },
      "source": [
        "## 2.3 Building a Persona Bot with In-Context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCtVPWC7jsnV"
      },
      "source": [
        "# Scratch\n",
        "This is where Daphne is keeping random code pieces which will not end up in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d-YoHX2wdDW"
      },
      "outputs": [],
      "source": [
        "engine.generate(\"What can\", top_p=1.0, num_tokens=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2QmogikgHut"
      },
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=\"hey what's going here\",\n",
        "      temperature=1.0,\n",
        "      max_tokens=24,\n",
        "      logprobs=1,\n",
        "      echo=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cCCZODQgHut"
      },
      "outputs": [],
      "source": [
        "offset = response[\"choices\"][0][\"logprobs\"][\"text_offset\"]\n",
        "logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "text = response[\"choices\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8bcUvGcgHuu"
      },
      "outputs": [],
      "source": [
        "for left, right, logprob in zip(offset, offset[1:] + [len(text)], logprobs):\n",
        "    print(\"token:\", text[left:right])\n",
        "    print(\"logprob\", logprob)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}